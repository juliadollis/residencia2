perfeito. abaixo está o pacote completo, do zero, com nome de diretório elip_igualpaper. ele replica a variante ELIP-C (CLIP) exatamente como no paper: backbone congelado, prompts visuais injetados no início do ViT (VPT-shallow), loss InfoNCE simétrica, hard sample mining Texto→Imagem global, e re-rank em duas etapas. inclui configurações para um teste rápido com 1000 amostras do pixparse/cc3m-wds e para o treino “real” com apple/DataCompDR-12M-bf16, além do push para o Hugging Face.

sobre épocas: Não sei. deixei parametrizado nas configs.

criar estrutura e instalar dependências

```bash
cd /workspace
mkdir -p elip_igualpaper/{configs,models,utils,training,infer,scripts}
touch elip_igualpaper/__init__.py elip_igualpaper/models/__init__.py elip_igualpaper/utils/__init__.py elip_igualpaper/training/__init__.py elip_igualpaper/infer/__init__.py

pip install -U "torch>=2.6" transformers datasets pillow sentencepiece faiss-cpu huggingface_hub safetensors tqdm webdataset
```

configs

```bash
cat > elip_igualpaper/configs/test_cc3m.yaml <<'YAML'
model_name: openai/clip-vit-base-patch32
img_size: 224
prompt_tokens: 12
seed: 3407
output_root: /workspace/elip_igualpaper/out_test_cc3m

dataset:
  name: pixparse/cc3m-wds
  split: train
  image_column: jpg
  text_column: txt
  text_is_list: false

max_train_examples: 1000
hardmine_subset: 1000
batch_size: 64
num_workers: 4
epochs: 1
lr: 5.0e-5
weight_decay: 0.01
fp16: true
grad_accum_steps: 1

rerank_k:
  coco: 100
  flickr: 100
  occluded_coco: 500
  imagenet_r: 1000

eval_split:
  name: pixparse/cc3m-wds
  split: train
  image_column: jpg
  text_column: txt
  text_is_list: false
  max_eval_examples: 500
YAML
```

```bash
cat > elip_igualpaper/configs/full_datacomp.yaml <<'YAML'
model_name: openai/clip-vit-base-patch32
img_size: 224
prompt_tokens: 12
seed: 3407
output_root: /workspace/elip_igualpaper/out_full_datacomp

dataset:
  name: apple/DataCompDR-12M-bf16
  split: train
  image_column: image
  text_column: caption
  text_is_list: false

max_train_examples: 6000000
hardmine_subset: 300000
batch_size: 64
num_workers: 4
epochs: 1
lr: 5.0e-5
weight_decay: 0.01
fp16: true
grad_accum_steps: 1

rerank_k:
  coco: 100
  flickr: 100
  occluded_coco: 500
  imagenet_r: 1000

eval_split:
  name: apple/DataCompDR-12M-bf16
  split: train
  image_column: image
  text_column: caption
  text_is_list: false
  max_eval_examples: 4000
YAML
```

models/mapper.py

```bash
cat > elip_igualpaper/models/mapper.py <<'PY'
import torch
class MLPMapper(torch.nn.Module):
    def __init__(self, in_dim: int, hidden: int, out_dim: int, num_tokens: int):
        super().__init__()
        self.num_tokens = num_tokens
        self.net = torch.nn.Sequential(
            torch.nn.Linear(in_dim, hidden),
            torch.nn.GELU(),
            torch.nn.Linear(hidden, hidden),
            torch.nn.GELU(),
            torch.nn.Linear(hidden, out_dim * num_tokens),
        )
    def forward(self, x):
        b = x.size(0)
        y = self.net(x)
        return y.view(b, self.num_tokens, -1)
PY
```

models/backbone_prompted_clip.py

```bash
cat > elip_igualpaper/models/backbone_prompted_clip.py <<'PY'
import torch
from transformers import CLIPModel, CLIPProcessor

class PromptedCLIP(torch.nn.Module):
    def __init__(self, model_name: str, image_size: int):
        super().__init__()
        self.model = CLIPModel.from_pretrained(
            model_name,
            torch_dtype=torch.float32,
            use_safetensors=True,
            low_cpu_mem_usage=True,
        )
        self.processor = CLIPProcessor.from_pretrained(model_name)
        for p in self.model.parameters():
            p.requires_grad = False
        self.image_size = image_size
        self.d_model = self.model.vision_model.config.hidden_size
        self.d_proj = self.model.config.projection_dim
    def encode_text_hidden(self, texts):
        inputs = self.processor(text=texts, padding=True, truncation=True, return_tensors="pt")
        for k in inputs:
            inputs[k] = inputs[k].to(self.model.device)
        out = self.model.get_text_features(**inputs)
        return torch.nn.functional.normalize(out, dim=-1)
    def encode_image_base(self, images):
        inputs = self.processor(images=images, return_tensors="pt")
        pixel_values = inputs["pixel_values"].to(self.model.device)
        out = self.model.get_image_features(pixel_values=pixel_values)
        return torch.nn.functional.normalize(out, dim=-1)
    def encode_image_guided(self, images, prompt_tokens):
        inputs = self.processor(images=images, return_tensors="pt")
        pixel_values = inputs["pixel_values"].to(self.model.device)
        vm = self.model.vision_model
        hs = vm.embeddings(pixel_values, interpolate_pos_encoding=True)
        cls = hs[:, :1, :]
        patches = hs[:, 1:, :]
        pt = prompt_tokens.to(hs.dtype)
        hs2 = torch.cat([cls, pt, patches], dim=1)
        enc_out = vm.encoder(hs2)
        cls_last = vm.post_layernorm(enc_out.last_hidden_state[:, 0, :])
        z = self.model.visual_projection(cls_last)
        return torch.nn.functional.normalize(z, dim=-1)
PY
```

utils/data.py

```bash
cat > elip_igualpaper/utils/data.py <<'PY'
import random, os
from typing import List, Tuple
from datasets import load_dataset
from PIL import Image

def _pick_first(x):
    if isinstance(x, list):
        return x[0] if len(x) > 0 else ""
    return x

def _to_rgb(x):
    if isinstance(x, Image.Image):
        return x.convert("RGB")
    if isinstance(x, dict) and "path" in x:
        return Image.open(x["path"]).convert("RGB")
    if isinstance(x, str) and os.path.exists(x):
        return Image.open(x).convert("RGB")
    return Image.open(x).convert("RGB")

def load_pairs(name: str, split: str, image_col: str, text_col: str, text_is_list: bool, max_examples=None, seed=3407) -> Tuple[List[Image.Image], List[str]]:
    ds = load_dataset(name, split=split, streaming=False)
    n = len(ds)
    idxs = list(range(n))
    random.Random(seed).shuffle(idxs)
    if max_examples is not None:
        idxs = idxs[:max_examples]
    images, texts = [], []
    for i in idxs:
        row = ds[i]
        txt = row[text_col]
        if text_is_list:
            txt = _pick_first(txt)
        images.append(_to_rgb(row[image_col]))
        texts.append(str(txt))
    return images, texts
PY
```

utils/hardmine.py

```bash
cat > elip_igualpaper/utils/hardmine.py <<'PY'
import torch, faiss
from tqdm import tqdm

@torch.no_grad()
def precompute_features(model, images, texts, img_bs=64, txt_bs=256, device="cuda"):
    print("Pré-computando features base. img_bs:", img_bs, "txt_bs:", txt_bs)
    z_img = []
    for i in tqdm(range(0, len(images), img_bs), desc="Embeddings de imagem base"):
        zi = model.encode_image_base(images[i:i+img_bs])
        z_img.append(zi.cpu())
    z_img = torch.cat(z_img, dim=0)
    z_txt = []
    for i in tqdm(range(0, len(texts), txt_bs), desc="Embeddings de texto base"):
        zt = model.encode_text_hidden(texts[i:i+txt_bs])
        z_txt.append(zt.cpu())
    z_txt = torch.cat(z_txt, dim=0)
    z_img = torch.nn.functional.normalize(z_img, dim=-1)
    z_txt = torch.nn.functional.normalize(z_txt, dim=-1)
    print("Shapes z_img:", z_img.shape, "z_txt:", z_txt.shape)
    return z_img, z_txt

def build_hard_batches_text2img(z_img, z_txt, batch_size=64, topn=63):
    print("Hard mining Texto→Imagem. batch_size:", batch_size, "topn:", topn)
    x = z_img.cpu().float().numpy()
    index = faiss.IndexFlatIP(x.shape[1])
    index.add(x)
    q = z_txt.cpu().float().numpy()
    _, I = index.search(q, topn + 1)
    batches = []
    for i in tqdm(range(I.shape[0]), desc="Minerando âncoras"):
        neigh = [j for j in I[i].tolist() if j != i][:topn]
        if len(neigh) < topn:
            continue
        batches.append([i] + neigh)
    print("Total de batches difíceis:", len(batches))
    return batches
PY
```

training/train_prompted_clip.py

```bash
cat > elip_igualpaper/training/train_prompted_clip.py <<'PY'
import os, yaml, torch, random, numpy as np
from pathlib import Path
from torch.utils.data import Dataset
from tqdm import tqdm
from safetensors.torch import save_file
from elip_igualpaper.models.backbone_prompted_clip import PromptedCLIP
from elip_igualpaper.models.mapper import MLPMapper
from elip_igualpaper.utils.data import load_pairs
from elip_igualpaper.utils.hardmine import precompute_features, build_hard_batches_text2img

class PairDataset(Dataset):
    def __init__(self, images, texts):
        self.images = images
        self.texts = texts
    def __len__(self):
        return len(self.images)
    def __getitem__(self, idx):
        return self.images[idx], self.texts[idx]

class InfoNCELoss(torch.nn.Module):
    def __init__(self, init_temp=0.07):
        super().__init__()
        self.logit_scale = torch.nn.Parameter(torch.log(torch.tensor(1.0/init_temp)))
    def forward(self, zi, zt):
        zi = torch.nn.functional.normalize(zi, dim=-1)
        zt = torch.nn.functional.normalize(zt, dim=-1)
        scale = self.logit_scale.exp().clamp(max=100)
        logits = scale * zi @ zt.t()
        labels = torch.arange(zi.size(0), device=zi.device)
        li = torch.nn.functional.cross_entropy(logits, labels)
        lt = torch.nn.functional.cross_entropy(logits.t(), labels)
        return 0.5*(li+lt)

def seed_all(s):
    random.seed(s)
    np.random.seed(s)
    torch.manual_seed(s)
    torch.cuda.manual_seed_all(s)

def _save_safetensors_mapper_crit(mapper, crit, path):
    state = {}
    for k, v in mapper.state_dict().items():
        state[f"mapper.{k}"] = v.detach().cpu()
    for k, v in crit.state_dict().items():
        state[f"crit.{k}"] = v.detach().cpu()
    save_file(state, str(path))

def main():
    os.environ["TOKENIZERS_PARALLELISM"] = "false"
    cfg_path = os.environ.get("ELIP_CFG", "elip_igualpaper/configs/test_cc3m.yaml")
    cfg = yaml.safe_load(open(cfg_path))
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    out_root = Path(cfg["output_root"]); out_root.mkdir(parents=True, exist_ok=True)
    seed_all(cfg["seed"])
    print("Config:", cfg_path)
    print("Dispositivo:", device)
    print("[1/7] Carregando CLIP")
    model = PromptedCLIP(cfg["model_name"], cfg["img_size"]).to(device).eval()
    print("d_model:", model.d_model, "d_proj:", model.d_proj)
    print("[2/7] Inicializando mapper e loss")
    mapper = MLPMapper(in_dim=model.d_proj, hidden=model.d_proj*2, out_dim=model.d_model, num_tokens=cfg["prompt_tokens"]).to(device)
    crit = InfoNCELoss().to(device)
    opt = torch.optim.AdamW(list(mapper.parameters())+list(crit.parameters()), lr=cfg["lr"], weight_decay=cfg["weight_decay"])
    ds = cfg["dataset"]
    print("[3/7] Carregando dataset", ds["name"], ds["split"])
    images, texts = load_pairs(ds["name"], ds["split"], ds["image_column"], ds["text_column"], ds["text_is_list"], max_examples=cfg.get("max_train_examples"))
    hm_subset = cfg.get("hardmine_subset")
    if hm_subset is not None and hm_subset < len(images):
        images = images[:hm_subset]; texts = texts[:hm_subset]
    print("Amostras para HM:", len(images))
    print("[4/7] Pré-computando embeddings base")
    z_img, z_txt = precompute_features(model, images, texts, img_bs=64, txt_bs=256, device=str(device))
    print("[5/7] Hard mining Texto→Imagem")
    batches = build_hard_batches_text2img(z_img, z_txt, batch_size=cfg["batch_size"], topn=cfg["batch_size"]-1)
    print("Batches:", len(batches))
    train_ds = PairDataset(images, texts)
    scaler = torch.amp.GradScaler('cuda') if (cfg.get("fp16", True) and torch.cuda.is_available()) else None
    steps_per_epoch = len(batches)
    print("[6/7] Treinando")
    print("Steps por época:", steps_per_epoch)
    for epoch in range(1, cfg["epochs"]+1):
        mapper.train(); crit.train()
        pbar = tqdm(range(steps_per_epoch), desc=f"Época {epoch}", total=steps_per_epoch, ncols=100)
        running = 0.0
        for step in pbar:
            idxs = batches[step]
            batch_imgs = [train_ds[i][0] for i in idxs]
            batch_txts = [train_ds[i][1] for i in idxs]
            with torch.no_grad():
                zt = model.encode_text_hidden(batch_txts)
            pt = mapper(zt.to(device))
            with torch.amp.autocast('cuda', enabled=(cfg.get("fp16", True) and torch.cuda.is_available())):
                zi = model.encode_image_guided(batch_imgs, pt)
                loss = crit(zi, zt.to(device))
            opt.zero_grad(set_to_none=True)
            if scaler is not None:
                scaler.scale(loss).backward()
                scaler.step(opt)
                scaler.update()
            else:
                loss.backward(); opt.step()
            running += float(loss.detach().cpu())
            pbar.set_postfix_str(f"loss_avg={running/(step+1):.4f}")
        ep_path = out_root / f"mapper_epoch{epoch}.safetensors"
        _save_safetensors_mapper_crit(mapper, crit, ep_path)
        print("Checkpoint salvo:", ep_path)
    last_path = out_root / "mapper_last.safetensors"
    _save_safetensors_mapper_crit(mapper, crit, last_path)
    print("[7/7] Concluído. Checkpoint final:", last_path)

if __name__ == "__main__":
    main()
PY
```

infer/infer_rerank_prompted_clip.py

```bash
cat > elip_igualpaper/infer/infer_rerank_prompted_clip.py <<'PY'
import os, yaml, torch, faiss
from pathlib import Path
from tqdm import tqdm
from collections import OrderedDict
from safetensors.torch import load_file
from elip_igualpaper.models.backbone_prompted_clip import PromptedCLIP
from elip_igualpaper.models.mapper import MLPMapper
from elip_igualpaper.utils.data import load_pairs

def build_index(x):
    x = torch.nn.functional.normalize(x, dim=-1).cpu().float().numpy()
    index = faiss.IndexFlatIP(x.shape[1])
    index.add(x)
    return index

def topk_index(index, q, k):
    q = torch.nn.functional.normalize(q, dim=-1).cpu().float().numpy()
    D, I = index.search(q, k)
    return I

def ranks_from_topk(I):
    n = I.shape[0]
    ranks = []
    for i in range(n):
        pos = 10**9
        for j in range(I.shape[1]):
            if I[i, j] == i:
                pos = j+1
                break
        ranks.append(pos)
    return ranks

def recall_at_k(ranks, k):
    return sum(1 for r in ranks if r<=k)/len(ranks)

def mrr(ranks):
    return sum(1.0/r for r in ranks)/len(ranks)

def ndcg_at_10(ranks):
    import math
    def dcg(rank):
        return 0.0 if rank>10 else 1.0/math.log2(rank+1)
    return sum(dcg(r) for r in ranks)/len(ranks)

def main():
    cfg_path = os.environ.get("ELIP_CFG", "elip_igualpaper/configs/test_cc3m.yaml")
    cfg = yaml.safe_load(open(cfg_path))
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    out_root = Path(cfg["output_root"])
    print("Inferência no dispositivo:", device)
    model = PromptedCLIP(cfg["model_name"], cfg["img_size"]).to(device).eval()
    print("Backbone pronto. d_model:", model.d_model, "d_proj:", model.d_proj)
    ckpts = sorted(out_root.glob("mapper_epoch*.safetensors"))
    if len(ckpts)==0:
        ckpts=[out_root/"mapper_last.safetensors"]
    print("Carregando checkpoint:", str(ckpts[-1]))
    state = load_file(str(ckpts[-1]))
    mp = OrderedDict((k.replace("mapper.",""), v) for k, v in state.items() if k.startswith("mapper."))
    mapper = MLPMapper(in_dim=model.d_proj, hidden=model.d_proj*2, out_dim=model.d_model, num_tokens=cfg["prompt_tokens"]).to(device)
    mapper.load_state_dict(mp, strict=True)
    mapper.eval()
    ev = cfg["eval_split"]
    images, texts = load_pairs(ev["name"], ev["split"], ev["image_column"], ev["text_column"], ev["text_is_list"], max_examples=ev.get("max_eval_examples"))
    with torch.no_grad():
        z_img_base = []
        bs = 64
        for i in tqdm(range(0,len(images),bs), desc="Index base"):
            zi = model.encode_image_base(images[i:i+bs])
            z_img_base.append(zi.cpu())
        z_img_base = torch.cat(z_img_base, dim=0)
        z_txt = []
        tb = 256
        for i in tqdm(range(0,len(texts),tb), desc="Textos"):
            zt = model.encode_text_hidden(texts[i:i+tb])
            z_txt.append(zt.cpu())
        z_txt = torch.cat(z_txt, dim=0)
    index = build_index(z_img_base)
    ds_name = ev["name"].lower()
    if "coco" in ds_name:
        k = cfg["rerank_k"]["coco"]
    elif "flickr" in ds_name:
        k = cfg["rerank_k"]["flickr"]
    elif "occluded" in ds_name:
        k = cfg["rerank_k"]["occluded_coco"]
    elif "imagenet" in ds_name or "image-net" in ds_name:
        k = cfg["rerank_k"]["imagenet_r"]
    else:
        k = 100
    n_images = z_img_base.shape[0]
    k_base = min(n_images, max(k, 1000))
    I_base = topk_index(index, z_txt, k_base)
    ranks_base = ranks_from_topk(I_base)
    r1_b = recall_at_k(ranks_base, 1)
    r5_b = recall_at_k(ranks_base, 5)
    r10_b = recall_at_k(ranks_base, 10)
    r50_b = recall_at_k(ranks_base, 50)
    mrr_b = mrr(ranks_base)
    ndcg_b = ndcg_at_10(ranks_base)
    I = I_base[:, :min(k, k_base)]
    guided_scores = torch.empty(I.shape[0], I.shape[1], device=device)
    with torch.no_grad():
        for i in tqdm(range(I.shape[0]), desc="Rerank"):
            pt = mapper(z_txt[i:i+1].to(device))
            ids = I[i].tolist()
            imgs = [images[j] for j in ids]
            zi = model.encode_image_guided(imgs, pt.expand(len(imgs), -1, -1))
            sim = (zi @ z_txt[i:i+1].to(device).t()).squeeze(1)
            guided_scores[i] = sim
    new_order = torch.argsort(guided_scores, dim=1, descending=True).cpu()
    I_tensor = torch.from_numpy(I)
    reranked = torch.gather(I_tensor, 1, new_order)
    ranks_re = []
    for i in range(reranked.shape[0]):
        pos = 10**9
        for j in range(reranked.shape[1]):
            if reranked[i, j].item() == i:
                pos = j+1
                break
        ranks_re.append(pos)
    r1_r = recall_at_k(ranks_re, 1)
    r5_r = recall_at_k(ranks_re, 5)
    r10_r = recall_at_k(ranks_re, 10)
    r50_r = recall_at_k(ranks_re, 50)
    mrr_r = mrr(ranks_re)
    ndcg_r = ndcg_at_10(ranks_re)
    print(f"Baseline R@1 {r1_b:.4f} R@5 {r5_b:.4f} R@10 {r10_b:.4f} R@50 {r50_b:.4f} MRR {mrr_b:.4f} nDCG@10 {ndcg_b:.4f}")
    print(f"Rerank   R@1 {r1_r:.4f} R@5 {r5_r:.4f} R@10 {r10_r:.4f} R@50 {r50_r:.4f} MRR {mrr_r:.4f} nDCG@10 {ndcg_r:.4f}")

if __name__ == "__main__":
    main()
PY
```

scripts/push_to_hf.py

```bash
cat > elip_igualpaper/scripts/push_to_hf.py <<'PY'
import os, yaml, json
from pathlib import Path
from safetensors.torch import load_file
from huggingface_hub import create_repo, upload_file

def main():
    repo_id = os.environ.get("HF_REPO_ID", "seu-usuario/elip-c-mapper")
    token = os.environ.get("HF_TOKEN", None)
    cfg_path = os.environ.get("ELIP_CFG", "elip_igualpaper/configs/test_cc3m.yaml")
    cfg = yaml.safe_load(open(cfg_path))
    out_dir = Path(cfg["output_root"])
    ckpt = out_dir/"mapper_last.safetensors"
    if token:
        create_repo(repo_id=repo_id, exist_ok=True, private=False, token=token)
    upload_file(path_or_fileobj=str(ckpt), path_in_repo="mapper_last.safetensors", repo_id=repo_id, token=token)
    upload_file(path_or_fileobj=cfg_path, path_in_repo="config.yaml", repo_id=repo_id, token=token)
    card = {"library_name":"ELIP-C","base_model":cfg["model_name"],"mapping_tokens":cfg["prompt_tokens"],"output_root":str(out_dir)}
    tmp = out_dir/"model_card.json"
    with open(tmp,"w") as f:
        json.dump(card,f)
    upload_file(path_or_fileobj=str(tmp), path_in_repo="model_card.json", repo_id=repo_id, token=token)
    print("ok")

if __name__=="__main__":
    main()
PY
```

como rodar — teste rápido (pixparse/cc3m-wds, 1000 amostras)

```bash
export PYTHONPATH=/workspace/elip_igualpaper:$PYTHONPATH
export ELIP_CFG=/workspace/elip_igualpaper/configs/test_cc3m.yaml
export PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True

python -m elip_igualpaper.training.train_prompted_clip
python -m elip_igualpaper.infer.infer_rerank_prompted_clip
```

como rodar — treino “real” (apple/DataCompDR-12M-bf16)

```bash
export PYTHONPATH=/workspace/elip_igualpaper:$PYTHONPATH
export ELIP_CFG=/workspace/elip_igualpaper/configs/full_datacomp.yaml
export PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True

python -m elip_igualpaper.training.train_prompted_clip
python -m elip_igualpaper.infer.infer_rerank_prompted_clip
```

push para o Hugging Face

```bash
huggingface-cli login
export HF_REPO_ID=seu-usuario/elip-c-mapper
export HF_TOKEN=$(cat ~/.cache/huggingface/token | head -n1 || echo "")
python -m elip_igualpaper.scripts.push_to_hf
```

onde, como e por que alterei

1. usei apenas safetensors para o CLIP e para checkpoints, evitando o CVE do torch.load e aderindo às boas práticas.
2. implementei hard sample mining global Texto→Imagem, com FAISS no espaço de imagens e consultas pelos textos, como na Seção 5.1.
3. preservei a arquitetura ELIP-C: backbone CLIP congelado, prompts visuais inseridos no início do ViT, MLP de mapeamento, InfoNCE simétrica, e re-rank top-k em duas etapas.
4. adicionei tqdm e prints em todas as fases para auditoria do pipeline.



perfeito. abaixo estão os arquivos completos e prontos para usar o dataset menor reach-vb/pokemon-blip-captions no teste. inclui um ajuste robusto em utils/data.py para autodetectar as colunas de imagem/texto caso o dataset use nomes diferentes, evitando erros.

criar a config de teste com o dataset de pokémon

```bash
cat > /workspace/elip_igualpaper/configs/test_pokemon.yaml <<'YAML'
model_name: openai/clip-vit-base-patch32
img_size: 224
prompt_tokens: 12
seed: 3407
output_root: /workspace/elip_igualpaper/out_test_pokemon

dataset:
  name: reach-vb/pokemon-blip-captions
  split: train
  image_column: image
  text_column: caption
  text_is_list: false

max_train_examples: 1000
hardmine_subset: 1000
batch_size: 64
num_workers: 4
epochs: 1
lr: 5.0e-5
weight_decay: 0.01
fp16: true
grad_accum_steps: 1

rerank_k:
  coco: 100
  flickr: 100
  occluded_coco: 500
  imagenet_r: 1000

eval_split:
  name: reach-vb/pokemon-blip-captions
  split: train
  image_column: image
  text_column: caption
  text_is_list: false
  max_eval_examples: 500
YAML
```

substituir utils/data.py por uma versão à prova de nomes de colunas

```bash
cat > /workspace/elip_igualpaper/utils/data.py <<'PY'
import random, os
from typing import List, Tuple
from datasets import load_dataset
from PIL import Image

def _pick_first(x):
    if isinstance(x, list):
        return x[0] if len(x) > 0 else ""
    return x

def _to_rgb(x):
    if isinstance(x, Image.Image):
        return x.convert("RGB")
    if isinstance(x, dict) and "path" in x:
        return Image.open(x["path"]).convert("RGB")
    if isinstance(x, str) and os.path.exists(x):
        return Image.open(x).convert("RGB")
    return Image.open(x).convert("RGB")

def _infer_columns(ds, image_col, text_col):
    cols = list(ds.features.keys())
    img_candidates = ["image", "img", "jpg", "png", "jpeg", "pil"]
    txt_candidates = ["caption", "text", "txt", "description", "prompt", "title"]
    if image_col not in cols:
        for c in img_candidates:
            if c in cols:
                image_col = c
                break
    if text_col not in cols:
        for c in txt_candidates:
            if c in cols:
                text_col = c
                break
    return image_col, text_col

def load_pairs(name: str, split: str, image_col: str, text_col: str, text_is_list: bool, max_examples=None, seed=3407) -> Tuple[List[Image.Image], List[str]]:
    ds = load_dataset(name, split=split, streaming=False)
    image_col, text_col = _infer_columns(ds, image_col, text_col)
    n = len(ds)
    idxs = list(range(n))
    random.Random(seed).shuffle(idxs)
    if max_examples is not None:
        idxs = idxs[:max_examples]
    images, texts = [], []
    for i in idxs:
        row = ds[i]
        txt = row[text_col]
        if text_is_list:
            txt = _pick_first(txt)
        images.append(_to_rgb(row[image_col]))
        texts.append(str(txt))
    return images, texts
PY
```

como rodar o teste com o dataset de pokémon

```bash
export PYTHONPATH=/workspace/elip_igualpaper:$PYTHONPATH
export ELIP_CFG=/workspace/elip_igualpaper/configs/test_pokemon.yaml
export TOKENIZERS_PARALLELISM=false
export PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True

python -m elip_igualpaper.training.train_prompted_clip
python -m elip_igualpaper.infer.infer_rerank_prompted_clip
```

se quiser já preparar um push do checkpoint de teste para o Hugging Face

```bash
huggingface-cli login
export HF_REPO_ID=seu-usuario/elip-c-mapper-test
export HF_TOKEN=$(huggingface-cli whoami -t)
export ELIP_CFG=/workspace/elip_igualpaper/configs/test_pokemon.yaml
python -m elip_igualpaper.scripts.push_to_hf
```

onde, como e por que alterei

1. adição da config test_pokemon.yaml usando reach-vb/pokemon-blip-captions com 1000 amostras e 1 época, mantendo exatamente o mesmo pipeline do paper.
2. atualização completa de utils/data.py para autodetectar colunas de imagem/texto quando os nomes divergirem, evitando KeyError em datasets menores.

pode rodar esses comandos direto. se aparecer qualquer log inesperado, manda aqui que ajusto.


cat > /workspace/elip_igualpaper/scripts/push_to_hf.py <<'PY'
import os, yaml
from pathlib import Path
from huggingface_hub import HfApi, create_repo, get_token, hf_hub_url, upload_file

def main():
    cfg_path = os.environ.get("ELIP_CFG", "elip_igualpaper/configs/test_pokemon.yaml")
    repo_id = os.environ.get("HF_REPO_ID", "").strip()
    if not repo_id:
        raise ValueError("Defina HF_REPO_ID, ex: usuario/elip-c-pokemon-test")
    token = os.environ.get("HF_TOKEN") or get_token()
    if not token:
        raise RuntimeError("Token ausente. Rode `hf auth login` ou exporte HF_TOKEN")

    cfg = yaml.safe_load(open(cfg_path))
    out_dir = Path(cfg["output_root"])
    out_dir.mkdir(parents=True, exist_ok=True)
    last_ckpt = out_dir / "mapper_last.safetensors"
    if not last_ckpt.exists():
        raise FileNotFoundError(str(last_ckpt))

    create_repo(repo_id=repo_id, repo_type="model", exist_ok=True, private=False, token=token)
    api = HfApi(token=token)

    def up(local, remote):
        upload_file(path_or_fileobj=str(local), path_in_repo=str(remote), repo_id=repo_id, repo_type="model", token=token)

    up(last_ckpt, "mapper_last.safetensors")
    for ep in sorted(out_dir.glob("mapper_epoch*.safetensors")):
        up(ep, ep.name)
    up(cfg_path, "config.yaml")

    readme = out_dir / "README.md"
    readme.write_text(
        f"# ELIP-C Mapper\nbase_model: {cfg['model_name']}\nprompt_tokens: {cfg['prompt_tokens']}\noutput_root: {cfg['output_root']}\n"
    )
    up(readme, "README.md")

    print("ok")
    print("repo:", repo_id)
    print("last:", hf_hub_url(repo_id, "mapper_last.safetensors", repo_type="model"))

if __name__ == "__main__":
    main()
PY

hf auth login
export HF_REPO_ID=turing552/elip-c-pokemon-test
export PYTHONPATH=/workspace/elip_igualpaper:$PYTHONPATH
export ELIP_CFG=/workspace/elip_igualpaper/configs/test_pokemon.yaml
python -m elip_igualpaper.scripts.push_to_hf




PARA RODAR IGUAL AO DO PAPER
Perfeito — vamos rodar “igual ao paper” para ELIP-C com DataCompDR-12M e as escolhas oficiais de k no re-ranking. Vou te passar:

1. a config completa do treino “oficial” (DataCompDR-12M) com os hiperparâmetros do paper para ELIP-C,
2. um runner para treinar e inferir,
3. o push para o Hub mantendo seu checkpoint de teste intacto.

Observações objetivas do paper que apliquei na config:

* Dataset de treino padrão: DataCompDR-12M (apple/DataCompDR-12M-bf16). 
* Batch size do ELIP-C: 40. 
* LR inicial do ELIP-C: 1e-3. 
* Re-ranking top-k (ELIP-C): COCO=100, Flickr=100, Occluded COCO=500, ImageNet-R=1000. 

Sobre épocas: não encontrei menção explícita de “número de épocas” no PDF enviado; o paper fixa batch size e LR e discute eficiência, mas não lista “N épocas” (se você tiver outra fonte, mando a config idêntica). Então deixei epochs configurável (default 1) — você pode aumentar depois.

### 1) Config “oficial” do paper (ELIP-C + DataCompDR-12M)

```bash
cat > /workspace/elip_igualpaper/configs/full_datacomp.yaml <<'YAML'
model_name: openai/clip-vit-base-patch32
img_size: 224
prompt_tokens: 12
seed: 3407
output_root: /workspace/elip_igualpaper/out_full_datacomp

dataset:
  name: apple/DataCompDR-12M-bf16
  split: train
  image_column: image
  text_column: caption
  text_is_list: false

max_train_examples: null
hardmine_subset: null
batch_size: 40
num_workers: 4
epochs: 1
lr: 1.0e-3
weight_decay: 0.01
fp16: true
grad_accum_steps: 1

rerank_k:
  coco: 100
  flickr: 100
  occluded_coco: 500
  imagenet_r: 1000

eval_split:
  name: laicsiifes/coco-captions-pt-br
  split: validation
  image_column: image
  text_column: captions
  text_is_list: true
  max_eval_examples: 5000
YAML
```

Se quiser avaliar também Flickr30k depois, troque `eval_split` para Flickr e mantenha os mesmos `rerank_k` (COCO e Flickr usam 100). As escolhas de k vêm do paper. 

### 2) Runner “oficial” (treino + inferência) com prints e tqdm

```bash
cat > /workspace/run_elip_oficial.sh <<'SH'
set -e
export PYTHONUNBUFFERED=1
export TOKENIZERS_PARALLELISM=false
export PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True
export PYTHONPATH=/workspace/elip_igualpaper:$PYTHONPATH
export ELIP_CFG=/workspace/elip_igualpaper/configs/full_datacomp.yaml

python -m elip_igualpaper.training.train_prompted_clip
python -m elip_igualpaper.infer.infer_rerank_prompted_clip

ls -lah /workspace/elip_igualpaper/out_full_datacomp
SH
bash /workspace/run_elip_oficial.sh
```

Se quiser validar primeiro em uma fração do DataComp (ex.: 200k amostras) sem mexer no resto, rode:

```bash
python - <<'PY'
import yaml
p="/workspace/elip_igualpaper/configs/full_datacomp.yaml"
y=yaml.safe_load(open(p))
y["max_train_examples"]=200000
y["hardmine_subset"]=200000
open(p,"w").write(yaml.safe_dump(y,sort_keys=False))
print("ok")
PY
bash /workspace/run_elip_oficial.sh
```

Depois volte para o “completo” setando `max_train_examples: null` e `hardmine_subset: null`.

### 3) Push do checkpoint “oficial” para o Hugging Face

Você já tem o script de push. Só ajustar as variáveis:

```bash
hf auth login
export HF_REPO_ID=turing552/elip-c-datacomp12m
export PYTHONPATH=/workspace/elip_igualpaper:$PYTHONPATH
export ELIP_CFG=/workspace/elip_igualpaper/configs/full_datacomp.yaml
python -m elip_igualpaper.scripts.push_to_hf
```

Se preferir forçar via env:

```bash
export HF_TOKEN=SEU_TOKEN_AQUI
export HF_REPO_ID=turing552/elip-c-datacomp12m
export PYTHONPATH=/workspace/elip_igualpaper:$PYTHONPATH
export ELIP_CFG=/workspace/elip_igualpaper/configs/full_datacomp.yaml
python -m elip_igualpaper.scripts.push_to_hf
```

### Onde, como e por quê alterei

* `full_datacomp.yaml`: fixei `batch_size: 40` e `lr: 1e-3` conforme a seção de Implementation Details do paper para ELIP-C; mantive o re-ranking `k` exatamente como especificado (100/100/500/1000). 
* `epochs`: deixei configurável; o PDF fornecido não traz número explícito de épocas, então mantive `1` para rodar já, e você pode aumentar depois.
* Avaliação default em COCO (5k caps/5k imgs) com Recall@k, como descrito na seção de avaliação; se quiser mAP para OOD (Occluded COCO / ImageNet-R), você pode alternar o split e o métrico depois. 

Se quiser, eu já te mando também uma `config_flickr.yaml` e uma `config_ood.yaml` com as métricas apropriadas; por ora foquei no “oficial” do ELIP-C em DataCompDR-12M do jeito que o paper descreve.



CORRIGIT PARA 
Perfeito. Aqui está a configuração corrigida para treinar exatamente como no paper: subset aleatório de 6M do DataCompDR-12M e avaliação em COCO em inglês.

Crie/substitua a config oficial:

```bash
cat > /workspace/elip_igualpaper/configs/full_datacomp.yaml <<'YAML'
model_name: openai/clip-vit-base-patch32
img_size: 224
prompt_tokens: 12
seed: 3407
output_root: /workspace/elip_igualpaper/out_full_datacomp

dataset:
  name: apple/DataCompDR-12M-bf16
  split: train
  image_column: image
  text_column: caption
  text_is_list: false

max_train_examples: 6000000
hardmine_subset: 6000000
batch_size: 40
num_workers: 4
epochs: 1
lr: 1.0e-3
weight_decay: 0.01
fp16: true
grad_accum_steps: 1

rerank_k:
  coco: 100
  flickr: 100
  occluded_coco: 500
  imagenet_r: 1000

eval_split:
  name: flax-community/coco2017-caption
  split: validation
  image_column: image
  text_column: captions
  text_is_list: true
  max_eval_examples: 5000
YAML
```

Rodar treino e inferência oficiais:

```bash
export PYTHONPATH=/workspace/elip_igualpaper:$PYTHONPATH
export ELIP_CFG=/workspace/elip_igualpaper/configs/full_datacomp.yaml
export TOKENIZERS_PARALLELISM=false
export PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True

python -m elip_igualpaper.training.train_prompted_clip
python -m elip_igualpaper.infer.infer_rerank_prompted_clip
```

Opcional: checar rapidamente se a config está com 6M e COCO em inglês:

```bash
python - <<'PY'
import yaml, pprint
y=yaml.safe_load(open("/workspace/elip_igualpaper/configs/full_datacomp.yaml"))
print("max_train_examples:", y["max_train_examples"])
print("hardmine_subset:", y["hardmine_subset"])
print("eval_split:", y["eval_split"]["name"], y["eval_split"]["split"])
PY
```

Onde, como e por que alterei

1. Substituí o eval_split para COCO em inglês, alinhando com a avaliação do paper.
2. Fixei max_train_examples e hardmine_subset em 6.000.000, reproduzindo o subset usado no paper para ELIP-C.
3. Mantive batch_size 40, LR 1e-3 e os valores de top-k no re-ranking iguais aos reportados.
